import boto3
import pandas as pd
import pyarrow.parquet as pq
import io
import json

def lambda_handler(event, context):
    # Define input variables
    target_prefix = event['target_prefix']  # The full S3 path in the consumption bucket to crawl
    target_account = event['target_account']  # The target AWS account ID
    target_database_name = event['target_database_name']  # The target Glue database name

    # Extract bucket name and prefix from the full S3 path
    if target_prefix.startswith("s3://"):
        target_prefix = target_prefix[len("s3://"):]
    source_bucket, target_prefix = target_prefix.split('/', 1)
    
    s3_client = boto3.client('s3')
    glue_client = boto3.client('glue')
    
    # List all Parquet files under the target prefix in the source bucket
    response = s3_client.list_objects_v2(Bucket=source_bucket, Prefix=target_prefix)
    
    parquet_files = [obj for obj in response.get('Contents', []) if obj['Key'].endswith('.parquet')]
    
    for file_obj in parquet_files:
        file_key = file_obj['Key']
        last_modified = file_obj['LastModified']
        
        # Check if the table already exists
        table_name = file_key.split('/')[-1].replace('.parquet', '')
        table_exists = False
        try:
            response = glue_client.get_table(DatabaseName=target_database_name, Name=table_name)
            table_exists = True
        except glue_client.exceptions.EntityNotFoundException:
            table_exists = False
        
        if table_exists:
            # Check if the file is updated
            table = response['Table']
            table_location = table['StorageDescriptor']['Location']
            
            # Extract the last modified date from the existing table location
            existing_file_key = table_location.split(f"s3://{source_bucket}/")[-1]
            existing_file_obj = s3_client.head_object(Bucket=source_bucket, Key=existing_file_key)
            existing_last_modified = existing_file_obj['LastModified']
            
            # If the file has not been updated, skip to the next file
            if last_modified <= existing_last_modified:
                continue
        
        # Read the Parquet file
        obj = s3_client.get_object(Bucket=source_bucket, Key=file_key)
        data = obj['Body'].read()
        table = pq.read_table(io.BytesIO(data))
        
        # Infer schema from the Parquet file
        schema = table.schema
        
        # Convert schema to Glue compatible format
        columns = []
        for field in schema:
            col_type = field.type
            col_name = field.name
            glue_type = 'string'  # Default to string
            
            if pd.api.types.is_integer_dtype(col_type):
                glue_type = 'int'
            elif pd.api.types.is_float_dtype(col_type):
                glue_type = 'double'
            elif pd.api.types.is_bool_dtype(col_type):
                glue_type = 'boolean'
            elif pd.api.types.is_datetime64_any_dtype(col_type):
                glue_type = 'timestamp'
            
            columns.append({'Name': col_name, 'Type': glue_type})
        
        # Create or update Glue table
        table_input = {
            'Name': table_name,
            'StorageDescriptor': {
                'Columns': columns,
                'Location': f"s3://{source_bucket}/{file_key}",
                'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',
                'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',
                'Compressed': False,
                'SerdeInfo': {
                    'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',
                    'Parameters': {}
                }
            },
            'TableType': 'EXTERNAL_TABLE'
        }
        
        if table_exists:
            glue_client.update_table(DatabaseName=target_database_name, TableInput=table_input)
        else:
            glue_client.create_table(DatabaseName=target_database_name, TableInput=table_input)
    
    return {
        'statusCode': 200,
        'body': json.dumps('Tables created or updated successfully')
    }