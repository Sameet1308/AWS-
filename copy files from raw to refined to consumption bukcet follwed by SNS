import boto3
import os
import logging
import pandas as pd
import io
import pyarrow as pa
import pyarrow.parquet as pq

# Initialize S3 client
s3_client = boto3.client('s3')

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Constants for the source and target S3 URIs and KMS key
SOURCE_S3_URI = 's3://source-bucket/b_q/subbq/'
TARGET_S3_URI = 's3://target-bucket/consumption/'
KMS_KEY_ID = 'your-kms-key-id'  # Replace with your KMS key ID

def parse_s3_uri(s3_uri):
    """Parses an S3 URI into bucket and prefix."""
    if not s3_uri.startswith("s3://"):
        raise ValueError("Invalid S3 URI")
    parts = s3_uri[5:].split("/", 1)
    bucket = parts[0]
    prefix = parts[1] if len(parts) > 1 else ""
    return bucket, prefix

def list_objects(bucket, prefix):
    """Lists objects in an S3 bucket with a given prefix."""
    paginator = s3_client.get_paginator('list_objects_v2')
    pages = paginator.paginate(Bucket=bucket, Prefix=prefix)

    objects = {}
    for page in pages:
        if 'Contents' in page:
            for obj in page['Contents']:
                objects[obj['Key']] = obj['LastModified']
    return objects

def convert_csv_to_parquet(file_obj):
    """Converts a CSV file to Parquet format in chunks."""
    parquet_buffer = io.BytesIO()
    writer = None

    for chunk in pd.read_csv(file_obj, chunksize=100000):
        table = pa.Table.from_pandas(chunk)
        if writer is None:
            writer = pq.ParquetWriter(parquet_buffer, table.schema)
        writer.write_table(table)

    if writer:
        writer.close()

    parquet_buffer.seek(0)
    return parquet_buffer

def convert_text_to_parquet(file_obj):
    """Converts a text file to Parquet format in chunks."""
    parquet_buffer = io.BytesIO()
    writer = None

    for chunk in pd.read_csv(file_obj, delimiter='\t', encoding='utf-8', chunksize=100000):
        table = pa.Table.from_pandas(chunk)
        if writer is None:
            writer = pq.ParquetWriter(parquet_buffer, table.schema)
        writer.write_table(table)

    if writer:
        writer.close()

    parquet_buffer.seek(0)
    return parquet_buffer

def convert_excel_to_parquet(file_obj):
    """Converts an Excel file to Parquet format."""
    df = pd.read_excel(file_obj)
    parquet_buffer = io.BytesIO()
    df.to_parquet(parquet_buffer, engine='pyarrow', index=False)
    parquet_buffer.seek(0)
    return parquet_buffer

def convert_file_to_parquet(file_obj, file_type):
    """Converts file content to Parquet format based on file type."""
    if file_type == 'csv':
        return convert_csv_to_parquet(file_obj)
    elif file_type == 'txt':
        return convert_text_to_parquet(file_obj)
    elif file_type == 'xlsx':
        return convert_excel_to_parquet(file_obj)
    else:
        raise ValueError(f"Unsupported file type: {file_type}")

def lambda_handler(event, context):
    source_bucket, source_prefix = parse_s3_uri(SOURCE_S3_URI)
    target_bucket, target_prefix = parse_s3_uri(TARGET_S3_URI)

    try:
        # List objects in the source and target prefixes
        source_objects = list_objects(source_bucket, source_prefix)
        target_objects = list_objects(target_bucket, target_prefix)

        for source_key, source_last_modified in source_objects.items():
            if not (source_key.endswith('.csv') or source_key.endswith('.txt') or source_key.endswith('.xlsx')):
                continue  # Skip unsupported file types

            relative_path = source_key[len(source_prefix):]
            file_extension = os.path.splitext(source_key)[1]
            file_name = os.path.basename(source_key).replace(file_extension, '.parquet')
            sub_prefix = os.path.join(target_prefix, os.path.dirname(relative_path))
            target_key = os.path.join(sub_prefix, file_name)

            # Check if the file exists in the target bucket and compare last modified times
            if target_key in target_objects:
                target_last_modified = target_objects[target_key]
                if source_last_modified <= target_last_modified:
                    logger.info(f"Skipping {source_key} as it is not modified since last copy")
                    continue

            # Get the file content from the source S3 bucket
            try:
                file_obj = s3_client.get_object(Bucket=source_bucket, Key=source_key)['Body']
                file_content = file_obj.read()
                file_obj = io.BytesIO(file_content)  # Reset the buffer for reading
            except Exception as e:
                logger.error(f"Error getting object {source_key} from bucket {source_bucket}: {e}")
                continue

            # Convert file to Parquet
            try:
                parquet_buffer = convert_file_to_parquet(file_obj, file_extension[1:])
            except Exception as e:
                logger.error(f"Error converting {file_extension} to Parquet for object {source_key}: {e}")
                continue

            # Upload the Parquet file to the target S3 bucket with KMS encryption
            try:
                s3_client.put_object(
                    Body=parquet_buffer,
                    Bucket=target_bucket,
                    Key=target_key,
                    ServerSideEncryption='aws:kms',
                    SSEKMSKeyId=KMS_KEY_ID
                )
                logger.info(f"Copied {source_key} to {target_bucket}/{target_key} as Parquet with KMS encryption")
            except Exception as upload_error:
                logger.error(f"Error uploading {source_key} to {target_bucket}/{target_key}: {upload_error}")

    except Exception as list_error:
        logger.error(f"Error listing objects in {source_bucket}/{source_prefix} or {target_bucket}/{target_prefix}: {list_error}")
        raise list_error