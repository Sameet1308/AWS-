import boto3
import json
from datetime import datetime, timezone
from pyarrow import parquet as pq
import os

s3_client = boto3.client('s3')
glue_client = boto3.client('glue')

# Define variables directly in the code
TARGET_ACCOUNT_ID = '17272'
DATABASE_NAME = 'yehjzjkksks'
CONSUMPTION_BUCKET_PREFIX = 's3://consumption-bucket/prefix/'

def infer_partition_columns(key, prefix):
    """Infer partition columns and their values from the S3 key."""
    partition_columns = []
    partition_values = []
    relative_path = key[len(prefix):]
    parts = relative_path.split('/')
    
    for part in parts[:-1]:  # Exclude the actual file name
        if '=' in part:
            col_name, col_value = part.split('=')
            partition_columns.append({'Name': col_name, 'Type': 'string'})
            partition_values.append(col_value)
    
    return partition_columns, partition_values

def lambda_handler(event, context):
    try:
        # Get bucket and object key from the event
        bucket = event['detail']['bucket']['name']
        key = event['detail']['object']['key']
        
        # Check if the object key matches the consumption bucket prefix
        if not key.startswith(CONSUMPTION_BUCKET_PREFIX.split('s3://')[1]):
            return {
                'statusCode': 400,
                'body': json.dumps('Object key does not match the consumption bucket prefix')
            }
        
        # Get the last modified time of the object
        response = s3_client.head_object(Bucket=bucket, Key=key)
        last_modified = response['LastModified']
        
        # Get the current time
        current_time = datetime.now(timezone.utc)
        
        # Check if the object was modified recently (within the last minute)
        if (current_time - last_modified).seconds <= 60:
            # Download the parquet file from S3
            local_file_path = '/tmp/temp.parquet'
            s3_client.download_file(bucket, key, local_file_path)
            
            # Infer schema using PyArrow
            table = pq.read_table(local_file_path)
            schema = table.schema

            # Convert PyArrow schema to Glue schema
            glue_columns = []
            for field in schema:
                glue_columns.append({
                    'Name': field.name,
                    'Type': str(field.type)
                })

            # Extract table name from the file name
            table_name = os.path.basename(key).split('.')[0]

            # Infer partition columns
            partition_columns, partition_values = infer_partition_columns(key, CONSUMPTION_BUCKET_PREFIX.split('s3://')[1])
            
            # Check if the table already exists
            try:
                glue_client.get_table(DatabaseName=DATABASE_NAME, Name=table_name)
                print(f"Table {table_name} already exists.")
                
                if partition_columns:
                    # Add partition to the existing table
                    glue_client.create_partition(
                        DatabaseName=DATABASE_NAME,
                        TableName=table_name,
                        PartitionInput={
                            'Values': partition_values,
                            'StorageDescriptor': {
                                'Columns': glue_columns,
                                'Location': f's3://{bucket}/{os.path.dirname(key)}',
                                'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',
                                'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',
                                'SerdeInfo': {
                                    'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
                                }
                            }
                        }
                    )
                    return {
                        'statusCode': 200,
                        'body': json.dumps(f'Partition added to table {table_name} successfully!')
                    }
                else:
                    return {
                        'statusCode': 200,
                        'body': json.dumps(f'Table {table_name} already exists and has no partitions.')
                    }
            except glue_client.exceptions.EntityNotFoundException:
                # Table does not exist, proceed with creation
                glue_client.create_table(
                    DatabaseName=DATABASE_NAME,
                    TableInput={
                        'Name': table_name,
                        'StorageDescriptor': {
                            'Columns': glue_columns,
                            'Location': f's3://{bucket}/{os.path.dirname(key)}',
                            'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',
                            'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',
                            'SerdeInfo': {
                                'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
                            }
                        },
                        'PartitionKeys': partition_columns if partition_columns else None,
                        'TableType': 'EXTERNAL_TABLE'
                    }
                )
                return {
                    'statusCode': 200,
                    'body': json.dumps(f'Table {table_name} created successfully with partitions.' if partition_columns else f'Table {table_name} created successfully without partitions.')
                }
        
        return {
            'statusCode': 200,
            'body': json.dumps('Table created or partition added successfully!')
        }
    
    except Exception as e:
        print(e)
        return {
            'statusCode': 500,
            'body': json.dumps('Error creating table or adding partition')
        }