import boto3
import pyarrow.parquet as pq
import logging

logging.basicConfig(level=logging.INFO)

def read_parquet_files(s3_prefix):
    s3 = boto3.client('s3')
    bucket_name = s3_prefix.split('/')[2]
    prefix = '/'.join(s3_prefix.split('/')[3:])
    
    try:
        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
        parquet_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.parquet')]
        return parquet_files
    except botocore.exceptions.ClientError as e:
        logging.error(f"Error listing objects in S3: {e}")
        return []

def infer_schema(s3_prefix, parquet_files):
    s3 = boto3.client('s3')
    bucket_name = s3_prefix.split('/')[2]
    
    if not parquet_files:
        logging.warning("No Parquet files found.")
        return None
    
    try:
        obj = s3.get_object(Bucket=bucket_name, Key=parquet_files[0])
        table = pq.read_table(obj['Body'])
        schema = table.schema
        return schema
    except Exception as e:
        logging.error(f"Error inferring schema: {e}")
        return None

def create_glue_table(schema, target_account_id, target_db_name, table_name):
    glue = boto3.client('glue', region_name='us-east-1')  # Adjust region as needed
    
    columns = []
    for field in schema:
        column = {
            'Name': field.name,
            'Type': str(field.type)
        }
        columns.append(column)
    
    try:
        response = glue.get_table(DatabaseName=target_db_name, Name=table_name)
        logging.info(f"Table {table_name} already exists in database {target_db_name}.")
    except glue.exceptions.EntityNotFoundException:
        logging.info(f"Table {table_name} does not exist. Creating table.")
        
        try:
            glue.create_table(
                DatabaseName=target_db_name,
                TableInput={
                    'Name': table_name,
                    'StorageDescriptor': {
                        'Columns': columns,
                        'Location': f's3://{s3_prefix}',
                        'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',
                        'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',
                        'SerdeInfo': {
                            'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',
                        },
                    },
                    'TableType': 'EXTERNAL_TABLE',
                }
            )
            logging.info(f"Table {table_name} created successfully.")
        except Exception as e:
            logging.error(f"Error creating table: {e}")

def main(s3_prefix, target_account_id, target_db_name):
    parquet_files = read_parquet_files(s3_prefix)
    if not parquet_files:
        logging.error("No Parquet files found to process.")
        return
    
    schema = infer_schema(s3_prefix, parquet_files)
    if not schema:
        logging.error("Could not infer schema from Parquet files.")
        return
    
    table_name = s3_prefix.split('/')[-1]
    create_glue_table(schema, target_account_id, target_db_name, table_name)

# Input parameters
s3_prefix = 's3://your-source-bucket/your-prefix/'
target_account_id = 'your-target-account-id'
target_db_name = 'your-target-db-name'

main(s3_prefix, target_account_id, target_db_name)
