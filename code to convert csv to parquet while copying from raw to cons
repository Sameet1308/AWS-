import boto3
import os
import logging
import pandas as pd
import io

# Initialize S3 client
s3_client = boto3.client('s3')

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Constants for the source and target S3 URIs and KMS key
SOURCE_S3_URI = 's3://source-bucket/b_q/subbq/'
TARGET_S3_URI = 's3://target-bucket/consumption/'
KMS_KEY_ID = 'your-kms-key-id'  # Replace with your KMS key ID

def parse_s3_uri(s3_uri):
    """Parses an S3 URI into bucket and prefix."""
    if not s3_uri.startswith("s3://"):
        raise ValueError("Invalid S3 URI")
    parts = s3_uri[5:].split("/", 1)
    bucket = parts[0]
    prefix = parts[1] if len(parts) > 1 else ""
    return bucket, prefix

def list_objects(bucket, prefix):
    """Lists objects in an S3 bucket with a given prefix."""
    paginator = s3_client.get_paginator('list_objects_v2')
    pages = paginator.paginate(Bucket=bucket, Prefix=prefix)

    objects = {}
    for page in pages:
        if 'Contents' in page:
            for obj in page['Contents']:
                objects[obj['Key']] = obj['LastModified']
    return objects

def convert_csv_to_parquet_streaming(csv_obj):
    """Converts CSV content to Parquet format using streaming."""
    chunksize = 10 ** 6  # Define a suitable chunk size
    parquet_buffer = io.BytesIO()
    writer = None

    for chunk in pd.read_csv(csv_obj, chunksize=chunksize):
        if writer is None:
            writer = pd.ExcelWriter(parquet_buffer, engine='parquet')
        chunk.to_parquet(writer, index=False, engine='pyarrow')

    writer.close()
    parquet_buffer.seek(0)
    return parquet_buffer

def lambda_handler(event, context):
    source_bucket, source_prefix = parse_s3_uri(SOURCE_S3_URI)
    target_bucket, target_prefix = parse_s3_uri(TARGET_S3_URI)

    try:
        # List objects in the source and target prefixes
        source_objects = list_objects(source_bucket, source_prefix)
        target_objects = list_objects(target_bucket, target_prefix)

        for source_key, source_last_modified in source_objects.items():
            if not source_key.endswith('.csv'):
                continue  # Skip non-CSV files

            relative_path = source_key[len(source_prefix):]
            file_name = os.path.basename(source_key).replace('.csv', '.parquet')
            sub_prefix = os.path.join(target_prefix, os.path.dirname(relative_path))
            target_key = os.path.join(sub_prefix, file_name)

            # Check if the file exists in the target bucket and compare last modified times
            if target_key in target_objects:
                target_last_modified = target_objects[target_key]
                if source_last_modified <= target_last_modified:
                    logger.info(f"Skipping {source_key} as it is not modified since last copy")
                    continue

            # Get the CSV content from the source S3 bucket
            try:
                csv_obj = s3_client.get_object(Bucket=source_bucket, Key=source_key)['Body']
            except Exception as e:
                logger.error(f"Error getting object {source_key} from bucket {source_bucket}: {e}")
                continue

            # Convert CSV to Parquet using streaming
            try:
                parquet_buffer = convert_csv_to_parquet_streaming(csv_obj)
            except Exception as e:
                logger.error(f"Error converting CSV to Parquet for object {source_key}: {e}")
                continue

            # Upload the Parquet file to the target S3 bucket with KMS encryption
            try:
                s3_client.put_object(
                    Body=parquet_buffer,
                    Bucket=target_bucket,
                    Key=target_key,
                    ServerSideEncryption='aws:kms',
                    SSEKMSKeyId=KMS_KEY_ID
                )
                logger.info(f"Copied {source_key} to {target_bucket}/{target_key} as Parquet with KMS encryption")
            except Exception as upload_error:
                logger.error(f"Error uploading {source_key} to {target_bucket}/{target_key}: {upload_error}")

    except Exception as list_error:
        logger.error(f"Error listing objects in {source_bucket}/{source_prefix} or {target_bucket}/{target_prefix}: {list_error}")
        raise list_error