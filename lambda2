import boto3
import pyarrow.parquet as pq
import os
import json
from datetime import datetime, timezone

# Configuration
LAST_EXECUTION_FILE = 'your-bucket/last_execution_lambda2.json'
account_id = '123456789012'
database_name = 'your-database-name'
consumption_prefix = 'target-bucket/consumption-data/'

s3 = boto3.client('s3')
glue_client = boto3.client('glue')

def get_last_execution_time():
    try:
        response = s3.get_object(Bucket=LAST_EXECUTION_FILE.split('/')[0], Key='/'.join(LAST_EXECUTION_FILE.split('/')[1:]))
        last_execution = json.loads(response['Body'].read())
        return datetime.fromisoformat(last_execution['last_execution_time'])
    except s3.exceptions.NoSuchKey:
        return datetime.min.replace(tzinfo=timezone.utc)

def update_last_execution_time():
    last_execution = {'last_execution_time': datetime.now(timezone.utc).isoformat()}
    s3.put_object(Bucket=LAST_EXECUTION_FILE.split('/')[0], Key='/'.join(LAST_EXECUTION_FILE.split('/')[1:]), Body=json.dumps(last_execution))

def lambda_handler(event, context):
    last_execution_time = get_last_execution_time()
    
    # List Parquet files in the consumption prefix
    consumption_bucket = consumption_prefix.split('/')[0]
    consumption_path = '/'.join(consumption_prefix.split('/')[1:])
    response = s3.list_objects_v2(Bucket=consumption_bucket, Prefix=consumption_path)
    files = response.get('Contents', [])

    for file in files:
        file_key = file['Key']
        last_modified = file['LastModified']

        # Process only new or modified files
        if last_modified <= last_execution_time:
            continue
        
        if not file_key.endswith('.parquet'):
            continue
        
        # Define the table name based on the file path
        table_name = file_key.split('/')[-1].replace('.parquet', '').replace('.', '_')

        # Define the target S3 location for the table
        table_location = f"s3://{consumption_bucket}/{os.path.dirname(file_key)}"

        # Retrieve the schema from the Parquet file
        obj = s3.get_object(Bucket=consumption_bucket, Key=file_key)
        table = pq.read_table(obj['Body'])
        schema = table.schema

        # Define Glue table columns based on the Parquet schema
        columns = [{'Name': field.name, 'Type': 'string'} for field in schema]  # Adjust type inference as needed

        # Check if the table already exists
        try:
            existing_table = glue_client.get_table(DatabaseName=database_name, Name=table_name)
            # Update the existing table with the new schema
            glue_client.update_table(
                DatabaseName=database_name,
                TableInput={
                    'Name': table_name,
                    'StorageDescriptor': {
                        'Columns': columns,
                        'Location': table_location,
                        'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',
                        'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',
                        'Compressed': True,
                        'NumberOfBuckets': -1,
                        'SerdeInfo': {
                            'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',
                        },
                        'BucketColumns': [],
                        'SortColumns': [],
                        'Parameters': {},
                        'SkewedInfo': {
                            'SkewedColumnNames': [],
                            'SkewedColumnValues': [],
                            'SkewedColumnValueLocationMaps': {}
                        },
                        'StoredAsSubDirectories': False
                    },
                    'TableType': 'EXTERNAL_TABLE',
                    'Parameters': {
                        'classification': 'parquet'
                    }
                }
            )
        except glue_client.exceptions.EntityNotFoundException:
            # Create the table if it does not exist
            glue_client.create_table(
                DatabaseName=database_name,
                TableInput={
                    'Name': table_name,
                    'StorageDescriptor': {
                        'Columns': columns,
                        'Location': table_location,
                        'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',
                        'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',
                        'Compressed': True,
                        'NumberOfBuckets': -1,
                        'SerdeInfo': {
                            'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',
                        },
                        'BucketColumns': [],
                        'SortColumns': [],
                        'Parameters': {},
                        'SkewedInfo': {
                            'SkewedColumnNames': [],
                            'SkewedColumnValues': [],
                            'SkewedColumnValueLocationMaps': {}
                        },
                        'StoredAsSubDirectories': False
                    },
                    'TableType': 'EXTERNAL_TABLE',
                    'Parameters': {
                        'classification': 'parquet'
                    }
                }
            )

    # Update the last execution time
    update_last_execution_time()

    return {
        'statusCode': 200,
        'body': f'Successfully processed files in Glue database {database_name}'
    }