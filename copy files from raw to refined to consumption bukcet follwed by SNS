import boto3
import os
import logging
import pandas as pd
import io

# Initialize S3 client
s3_client = boto3.client('s3')

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Constants for the source and target S3 URIs and KMS key
SOURCE_S3_URI = 's3://source-bucket/b_q/subbq/'
TARGET_S3_URI = 's3://target-bucket/consumption/'
KMS_KEY_ID = 'your-kms-key-id'  # Replace with your KMS key ID

def parse_s3_uri(s3_uri):
    """Parses an S3 URI into bucket and prefix."""
    if not s3_uri.startswith("s3://"):
        raise ValueError("Invalid S3 URI")
    parts = s3_uri[5:].split("/", 1)
    bucket = parts[0]
    prefix = parts[1] if len(parts) > 1 else ""
    return bucket, prefix

def list_objects(bucket, prefix):
    """Lists objects in an S3 bucket with a given prefix."""
    paginator = s3_client.get_paginator('list_objects_v2')
    pages = paginator.paginate(Bucket=bucket, Prefix=prefix)

    objects = {}
    for page in pages:
        if 'Contents' in page:
            for obj in page['Contents']:
                objects[obj['Key']] = obj['LastModified']
    return objects

def s3_select_csv(bucket, key):
    """Uses S3 Select to read and parse a CSV file from S3."""
    response = s3_client.select_object_content(
        Bucket=bucket,
        Key=key,
        ExpressionType='SQL',
        Expression='SELECT * FROM S3Object',
        InputSerialization={'CSV': {'FileHeaderInfo': 'USE'}},
        OutputSerialization={'CSV': {}},
    )

    records = []
    for event in response['Payload']:
        if 'Records' in event:
            records.append(event['Records']['Payload'].decode('utf-8'))
        elif 'Stats' in event:
            logger.info(f"Processed {event['Stats']['Details']['BytesProcessed']} bytes")

    return ''.join(records)

def convert_csv_to_parquet(csv_content):
    """Converts CSV content to Parquet format."""
    csv_buffer = io.StringIO(csv_content)
    df = pd.read_csv(csv_buffer)
    df = remove_spaces_from_headers(df)
    parquet_buffer = io.BytesIO()
    df.to_parquet(parquet_buffer, index=False, engine='pyarrow')
    parquet_buffer.seek(0)
    return parquet_buffer

def remove_spaces_from_headers(df):
    """Removes spaces from DataFrame column headers."""
    df.columns = df.columns.str.replace(' ', '_')
    return df

def lambda_handler(event, context):
    source_bucket, source_prefix = parse_s3_uri(SOURCE_S3_URI)
    target_bucket, target_prefix = parse_s3_uri(TARGET_S3_URI)

    try:
        # List objects in the source and target prefixes
        source_objects = list_objects(source_bucket, source_prefix)
        target_objects = list_objects(target_bucket, target_prefix)

        for source_key, source_last_modified in source_objects.items():
            if not source_key.endswith('.csv'):
                continue  # Skip non-CSV files

            relative_path = source_key[len(source_prefix):]
            file_name = os.path.basename(source_key).replace('.csv', '.parquet')
            target_key = os.path.join(target_prefix, relative_path).replace('.csv', '.parquet')

            # Check if the file exists in the target bucket and compare last modified times
            if target_key in target_objects:
                target_last_modified = target_objects[target_key]
                if source_last_modified <= target_last_modified:
                    logger.info(f"Skipping {source_key} as it is not modified since last copy")
                    continue

            # Get the CSV content from the source S3 bucket using S3 Select
            try:
                csv_content = s3_select_csv(source_bucket, source_key)
            except Exception as e:
                logger.error(f"Error selecting object {source_key} from bucket {source_bucket}: {e}")
                continue

            # Convert CSV to Parquet
            try:
                parquet_buffer = convert_csv_to_parquet(csv_content)
            except Exception as e:
                logger.error(f"Error converting CSV to Parquet for object {source_key}: {e}")
                continue

            # Upload the Parquet file to the target S3 bucket with KMS encryption
            try:
                s3_client.put_object(
                    Body=parquet_buffer,
                    Bucket=target_bucket,
                    Key=target_key,
                    ServerSideEncryption='aws:kms',
                    SSEKMSKeyId=KMS_KEY_ID
                )
                logger.info(f"Copied {source_key} to {target_bucket}/{target_key} as Parquet with KMS encryption")
            except Exception as upload_error:
                logger.error(f"Error uploading {source_key} to {target_bucket}/{target_key}: {upload_error}")

    except Exception as list_error:
        logger.error(f"Error listing objects in {source_bucket}/{source_prefix} or {target_bucket}/{target_prefix}: {list_error}")
        raise list_error