import json
import boto3
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import io

s3 = boto3.client('s3')
kms_key_id = 'your-kms-key-id'

def lambda_handler(event, context):
    # Parse the S3 event
    for record in event['Records']:
        source_bucket = record['s3']['bucket']['name']
        source_key = record['s3']['object']['key']
        
        # Define target bucket and prefix
        target_bucket = 'your-target-bucket'
        source_prefix = 'your/source/prefix/'
        target_prefix = 'your/target/prefix/'
        
        # Only process CSV files
        if source_key.endswith('.csv'):
            copy_and_convert_file(source_bucket, source_key, target_bucket, source_prefix, target_prefix)

    return {
        'statusCode': 200,
        'body': json.dumps('Files processed successfully.')
    }

def copy_and_convert_file(source_bucket, source_key, target_bucket, source_prefix, target_prefix):
    # Download CSV file from source S3 bucket
    csv_obj = s3.get_object(Bucket=source_bucket, Key=source_key)
    csv_data = csv_obj['Body'].read().decode('utf-8')
    
    # Convert CSV to Parquet
    df = pd.read_csv(io.StringIO(csv_data))
    table = pa.Table.from_pandas(df)
    
    # Write Parquet file to in-memory buffer
    buffer = io.BytesIO()
    pq.write_table(table, buffer)
    buffer.seek(0)
    
    # Determine target key maintaining the folder structure
    relative_key = source_key[len(source_prefix):]
    target_key = target_prefix + relative_key.replace('.csv', '.parquet')

    # Upload Parquet file to target S3 bucket with KMS encryption
    s3.put_object(
        Bucket=target_bucket,
        Key=target_key,
        Body=buffer.getvalue(),
        ServerSideEncryption='aws:kms',
        SSEKMSKeyId=kms_key_id
    )
